{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c5cadbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "\n",
    "import pandas\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9ca2b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def scale_resize_image(image):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) # equivalent to dividing image pixels by 255\n",
    "    image = tf.image.resize(image, (112, 112)) # Resizing the image to 224x224 dimention\n",
    "    return (image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e338e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_CNN(part, nb_parts):\n",
    "    \n",
    "    begin_index = int((1890/nb_parts)*part)\n",
    "    middle_index = int(((1890/nb_parts)/4)*3)\n",
    "    end_index = int((1890/nb_parts)*(part + 1))\n",
    "    \n",
    "    print(\"begin_index: \" + str(begin_index))\n",
    "    print(\"middle_index: \" + str(middle_index))\n",
    "    print(\"end_index: \" + str(end_index))\n",
    "    \n",
    "    batch_size = end_index - begin_index\n",
    "    \n",
    "    print(\"batch_size: \" + str(batch_size))\n",
    "    \n",
    "    \n",
    "    # HANDLE IMPORTS\n",
    "\n",
    "    import pandas\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    \n",
    "    # DOWNLOAD DATA FROM CSV\n",
    "\n",
    "    images = pandas.read_csv('Data/an_art_images.csv')\n",
    "    data = pandas.read_csv('Data/an_art_block_data.csv')\n",
    "    \n",
    "    # CONVERT DATA TO NUMPY ARRAYS\n",
    "\n",
    "    images_array = np.zeros((batch_size,112,112,3))\n",
    "    scores_array = np.zeros((batch_size,1))\n",
    "    \n",
    "    # IMAGES\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        image_location = images[['image_filename']].values[begin_index + i][0]\n",
    "        im = Image.open('Data/Images/Final_resized_selection_batch1/' + image_location)\n",
    "        im = np.array(im)\n",
    "        im = scale_resize_image(im)\n",
    "        images_array[i] = im\n",
    "        \n",
    "    # SCORES\n",
    "\n",
    "    avg_data = data.groupby(['image_id']).mean()\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        scores_array[i] = avg_data[['response']].values[begin_index + i][0]\n",
    "\n",
    "    abs_max = np.amax(np.abs(scores_array))\n",
    "    scores_array = scores_array/abs_max\n",
    "    \n",
    "    # SPLIT DATA IN TWO PARTS FOR TRAINING\n",
    "\n",
    "    scores_array_train = scores_array[:middle_index]\n",
    "    images_array_train = images_array[:middle_index]\n",
    "\n",
    "    scores_array_val = scores_array[middle_index:]\n",
    "    images_array_val = images_array[middle_index:]\n",
    "    \n",
    "    print(\"scores_array_val.shape: \" + str(scores_array_val.shape))\n",
    "    print(\"images_array_val.shape: \" + str(images_array_val.shape))\n",
    "    \n",
    "    \n",
    "    return (scores_array_train,images_array_train), (scores_array_val, images_array_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df77895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_CNN():\n",
    "    begin_index = 1890\n",
    "        \n",
    "    # HANDLE IMPORTS\n",
    "\n",
    "    import pandas\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    \n",
    "    # DOWNLOAD DATA FROM CSV\n",
    "\n",
    "    images = pandas.read_csv('Data/an_art_images.csv')\n",
    "    data = pandas.read_csv('Data/an_art_block_data.csv')\n",
    "    \n",
    "    # CONVERT DATA TO NUMPY ARRAYS\n",
    "\n",
    "    images_array = np.zeros((100,112,112,3))\n",
    "    scores_array = np.zeros((100,1))\n",
    "    \n",
    "    # IMAGES\n",
    "\n",
    "    for i in range(100):\n",
    "        image_location = images[['image_filename']].values[begin_index + i][0]\n",
    "        im = Image.open('Data/Images/Final_resized_selection_batch1/' + image_location)\n",
    "        im = np.array(im)\n",
    "        im = scale_resize_image(im)\n",
    "        images_array[i] = im\n",
    "        \n",
    "    # SCORES\n",
    "\n",
    "    avg_data = data.groupby(['image_id']).mean()\n",
    "\n",
    "    for i in range(100):\n",
    "        scores_array[i] = avg_data[['response']].values[begin_index + i][0]\n",
    "\n",
    "    abs_max = np.amax(np.abs(scores_array))\n",
    "    scores_array = scores_array/abs_max\n",
    "    \n",
    "    return (scores_array,images_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a8e5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_SOM_pretrain():\n",
    "    \n",
    "    # HANDLE IMPORTS\n",
    "    \n",
    "    from sklearn.cluster import DBSCAN\n",
    "    import pandas\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    \n",
    "     # DOWNLOAD DATA FROM CSV\n",
    "\n",
    "    participants = pandas.read_csv('Data/participant_table.csv')\n",
    "    images = pandas.read_csv('Data/an_art_images.csv')\n",
    "    data = pandas.read_csv('Data/an_art_block_data.csv')\n",
    "    \n",
    "    # CREATE CLUSTER ARRAY\n",
    "    \n",
    "    cluster_array = np.zeros((34, 1990))\n",
    "    \n",
    "    for i in range(20160): # for each response, check participant and painting and add to cluster_array\n",
    "        participant_index = int((data[['participant_id']].values[i][0])/3 - 1)\n",
    "        image_index = int((data[['image_id']].values[i][0])/3 - 1)\n",
    "        cluster_array[participant_index][image_index] = data[['response']].values[i][0]\n",
    "    \n",
    "    abs_max = np.amax(np.abs(cluster_array))\n",
    "    cluster_array = cluster_array/abs_max\n",
    "    cluster_array[np.isnan(cluster_array)]=0.5 # replace NaN values with average\n",
    "    \n",
    "    return cluster_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86b10233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b, c):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p], c[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d41357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_SOM_network_final():\n",
    "    \n",
    "    batch_size = int(20160)\n",
    "    \n",
    "    begin_index = 0\n",
    "    middle_index = int((batch_size/4)*3)\n",
    "    end_index = batch_size - 250\n",
    "    \n",
    "    # HANDLE IMPORTS\n",
    "    \n",
    "    from sklearn.cluster import DBSCAN\n",
    "    import pandas\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    \n",
    "     # DOWNLOAD DATA FROM CSV\n",
    "\n",
    "    participants = pandas.read_csv('Data/participant_table.csv')\n",
    "    images = pandas.read_csv('Data/an_art_images.csv')\n",
    "    data = pandas.read_csv('Data/an_art_block_data.csv')\n",
    "    \n",
    "    # CREATE CLUSTER ARRAY\n",
    "    \n",
    "    cluster_array = np.zeros((34, 1990))\n",
    "    \n",
    "    for i in range(20160): # for each response, check participant and painting and add to cluster_array\n",
    "        participant_index = int((data[['participant_id']].values[i][0])/3 - 1)\n",
    "        image_index = int((data[['image_id']].values[i][0])/3 - 1)\n",
    "        cluster_array[participant_index][image_index] = data[['response']].values[i][0]\n",
    "    \n",
    "    #abs_max = np.amax(np.abs(cluster_array))\n",
    "    #cluster_array = cluster_array/abs_max\n",
    "    cluster_array = (cluster_array)/(np.max(cluster_array))\n",
    "    cluster_array[np.isnan(cluster_array)]=0.1 # replace NaN values with average\n",
    "    \n",
    "    # CONVERT DATA TO NUMPY ARRAYS\n",
    "    \n",
    "    rating_array = np.zeros((batch_size,1))\n",
    "    image_array = np.zeros((batch_size,112,112,3))\n",
    "    clusterinfo_array = np.zeros((batch_size,1990))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "    \n",
    "        #ratings\n",
    "\n",
    "        rating_array[i] = (data[['response']].values[begin_index + i][0])\n",
    "\n",
    "\n",
    "        #images\n",
    "\n",
    "        image_index = int((data[['image_id']].values[begin_index + i][0])/3 - 1)\n",
    "        image_location = images[['image_filename']].values[image_index][0]\n",
    "        im = Image.open('Data/Images/Final_resized_selection_batch1/' + image_location)\n",
    "        im = np.array(im)\n",
    "        im = scale_resize_image(im)\n",
    "        image_array[i] = im\n",
    "\n",
    "        #clusters\n",
    "\n",
    "        participant_index = int((data[['participant_id']].values[begin_index + i][0])/3 - 1)\n",
    "        clusterinfo_array[i] = cluster_array[participant_index]\n",
    "        \n",
    "    rating_array[np.isnan(rating_array)] = 0.1\n",
    "    image_array[np.isnan(image_array)] = 0.1\n",
    "\n",
    "    #abs_max = np.amax(np.abs(rating_array))\n",
    "    #rating_array = rating_array/abs_max\n",
    "    rating_array = (rating_array)/(np.max(rating_array))\n",
    "    \n",
    "    clusterinfo_array[np.isnan(clusterinfo_array)]=0.5 # replace NaN values with average\n",
    "    rating_array[np.isnan(rating_array)]=0.5\n",
    "    image_array[np.isnan(image_array)]=0.5\n",
    "    \n",
    "    # SPLIT DATA INTO PARTS FOR TRAINING\n",
    "    \n",
    "    rating_array_train = rating_array[:middle_index]\n",
    "    image_array_train = image_array[:middle_index]\n",
    "    cluster_array_train = clusterinfo_array[:middle_index]\n",
    "\n",
    "    rating_array_val = rating_array[middle_index:end_index]\n",
    "    image_array_val = image_array[middle_index:end_index]\n",
    "    cluster_array_val = clusterinfo_array[middle_index:end_index]\n",
    "    \n",
    "    rating_array_test = rating_array[end_index:]\n",
    "    image_array_test = image_array[end_index:]\n",
    "    cluster_array_test = clusterinfo_array[end_index:]\n",
    "    \n",
    "    return [cluster_array_train,image_array_train,rating_array_train], [cluster_array_val,image_array_val,rating_array_val], [cluster_array_test, image_array_test, rating_array_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aa8a4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_manual_cluster_network_final():\n",
    "    \n",
    "    batch_size = int(20160)\n",
    "    \n",
    "    begin_index = 0\n",
    "    middle_index = int((batch_size/4)*3)\n",
    "    end_index = batch_size - 250\n",
    "    \n",
    "    \n",
    "    # HANDLE IMPORTS\n",
    "    \n",
    "    from sklearn.cluster import DBSCAN\n",
    "    import pandas\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from PIL import Image\n",
    "    \n",
    "     # DOWNLOAD DATA FROM CSV\n",
    "\n",
    "    participants = pandas.read_csv('Data/participant_table.csv')\n",
    "    images = pandas.read_csv('Data/an_art_images.csv')\n",
    "    data = pandas.read_csv('Data/an_art_block_data.csv')\n",
    "    \n",
    "    # CREATE CLUSTER ARRAY\n",
    "    \n",
    "    cluster_array = np.zeros((34, 1990))\n",
    "    \n",
    "    for i in range(20160): # for each response, check participant and painting and add to cluster_array\n",
    "        participant_index = int((data[['participant_id']].values[i][0])/3 - 1)\n",
    "        image_index = int((data[['image_id']].values[i][0])/3 - 1)\n",
    "        cluster_array[participant_index][image_index] = data[['response']].values[i][0]\n",
    "        \n",
    "    cluster_array[np.isnan(cluster_array)]=0.0\n",
    "    \n",
    "    abs_max = np.amax(np.abs(cluster_array))\n",
    "    cluster_array = cluster_array/abs_max\n",
    "    cluster_array[np.isnan(cluster_array)]=0.5 # replace NaN values with average\n",
    "    \n",
    "    clustering = DBSCAN(eps=12, min_samples=2).fit(cluster_array)\n",
    "    \n",
    "    # CONVERT DATA TO NUMPY ARRAYS\n",
    "    \n",
    "    rating_array = np.zeros((batch_size,1))\n",
    "    image_array = np.zeros((batch_size,112,112,3))\n",
    "    clusterinfo_array = np.zeros((batch_size,1))\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "    \n",
    "        #ratings\n",
    "\n",
    "        rating_array[i] = (data[['response']].values[begin_index + i][0])\n",
    "\n",
    "\n",
    "        #images\n",
    "\n",
    "        image_index = int((data[['image_id']].values[begin_index + i][0])/3 - 1)\n",
    "        image_location = images[['image_filename']].values[image_index][0]\n",
    "        im = Image.open('Data/Images/Final_resized_selection_batch1/' + image_location)\n",
    "        im = np.array(im)\n",
    "        im = scale_resize_image(im)\n",
    "        image_array[i] = im\n",
    "\n",
    "        #clusters\n",
    "\n",
    "        participant_index = int((data[['participant_id']].values[begin_index + i][0])/3 - 1)\n",
    "        clusterinfo_array[i] = clustering.labels_[participant_index]\n",
    "        \n",
    "    # REPLACE ZEROES WITH SMALL NUMBERS\n",
    "    \n",
    "    rating_array = np.where(rating_array == 0, 0.1, rating_array)\n",
    "    image_array = np.where(image_array == 0, 0.1, image_array)\n",
    "    rating_array[np.isnan(rating_array)] = 0.1\n",
    "    image_array[np.isnan(image_array)] = 0.1\n",
    "    \n",
    "    abs_max = np.amax(np.abs(clusterinfo_array))\n",
    "    clusterinfo_array = np.abs(clusterinfo_array)/abs_max\n",
    "    rating_array = (rating_array)/(np.max(rating_array))\n",
    "    #abs_max = np.amax(np.abs(rating_array))\n",
    "    #rating_array = rating_array/abs_max\n",
    "    \n",
    "    clusterinfo_array[np.isnan(clusterinfo_array)]=0.5 # replace NaN values with average\n",
    "    rating_array[np.isnan(rating_array)]=0.5\n",
    "    image_array[np.isnan(image_array)]=0.5\n",
    "    \n",
    "    # SHUFFLE DATA\n",
    "    \n",
    "    [rating_array, image_array, clusterinfo_array] = unison_shuffled_copies(rating_array, image_array, clusterinfo_array)\n",
    "    \n",
    "    # SPLIT DATA INTO PARTS FOR TRAINING\n",
    "    \n",
    "    rating_array_train = rating_array[:middle_index]\n",
    "    image_array_train = image_array[:middle_index]\n",
    "    cluster_array_train = clusterinfo_array[:middle_index]\n",
    "\n",
    "    rating_array_val = rating_array[middle_index:end_index]\n",
    "    image_array_val = image_array[middle_index:end_index]\n",
    "    cluster_array_val = clusterinfo_array[middle_index:end_index]\n",
    "    \n",
    "    rating_array_test = rating_array[end_index:]\n",
    "    image_array_test = image_array[end_index:]\n",
    "    cluster_array_test = clusterinfo_array[end_index:]\n",
    "    \n",
    "    return [cluster_array_train,image_array_train,rating_array_train], [cluster_array_val,image_array_val,rating_array_val], [cluster_array_test, image_array_test, rating_array_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e887d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
