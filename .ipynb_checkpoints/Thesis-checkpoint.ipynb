{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b095b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9622a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90df2441",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86394c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44927ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "461934f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a420d3",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a95853a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "participants = pandas.read_csv('Data/participant_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c3cdea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pandas.read_csv('Data/an_art_images.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87364a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pandas.read_csv('Data/an_art_block_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6f0968c",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pandas.read_csv('Data/an_art_block_table.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41021ffa",
   "metadata": {},
   "source": [
    "# CONVOLUTIONAL NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b40fae",
   "metadata": {},
   "source": [
    "For the normal convolutional neural network, the network that will be used is the VGG16 network which is pretrained on the imagenet dataset. This can be used in keras as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "245f597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e2860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 512)               14714688  \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 14,715,201\n",
      "Trainable params: 14,715,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(VGG16(weights = 'imagenet', include_top=False, input_shape=(224,224,3), pooling='avg'))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd5eeb8",
   "metadata": {},
   "source": [
    "First we need to fix the data so that it can be used as input for the network. This requires an array of tuples of images and scores. To get this array we need the average score for each image. This means linking the image at the location specified in the images dataset to the average of all responses in the data dataset corresponding to the same image id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d7f98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a67eb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_array = np.zeros((1990,224,224,3))\n",
    "scores_array = np.zeros((1990,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dc23983",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def scale_resize_image(image):\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32) # equivalent to dividing image pixels by 255\n",
    "    image = tf.image.resize(image, (224, 224)) # Resizing the image to 224x224 dimention\n",
    "    return (image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eb11aa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 6 calls to <function scale_resize_image at 0x000002BC191D2D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 7 calls to <function scale_resize_image at 0x000002BC191D2D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "for i in range(1990):\n",
    "    image_location = images[['image_filename']].values[i][0]\n",
    "    im = Image.open('Data/Images/Final_resized_selection_batch1/' + image_location)\n",
    "    im = np.array(im)\n",
    "    im = scale_resize_image(im)\n",
    "    images_array[i] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1eb358d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "avg_data = data.groupby(['image_id']).mean()\n",
    "\n",
    "for i in range(1990):\n",
    "    scores_array[i] = avg_data[['response']].values[i][0]\n",
    "    \n",
    "abs_max = np.amax(np.abs(scores_array))\n",
    "scores_array = scores_array/abs_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f136778",
   "metadata": {},
   "source": [
    "The data must be split into train and test sets, and it must also be shuffled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb20a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "scores_array_shuffled, images_array_shuffled = unison_shuffled_copies(scores_array, images_array)\n",
    "\n",
    "scores_array_train = scores_array_shuffled[:1400]\n",
    "images_array_train = images_array_shuffled[:1400]\n",
    "\n",
    "scores_array_val = scores_array_shuffled[1400:1500]\n",
    "images_array_val = images_array_shuffled[1400:1500]\n",
    "\n",
    "scores_array_test = scores_array_shuffled[1500:]\n",
    "images_array_test = images_array_shuffled[1500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1e72c0",
   "metadata": {},
   "source": [
    "Then the model can be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2cb5ee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "700/700 [==============================] - 37s 53ms/step - loss: 0.0249 - val_loss: 0.0217- ET - ETA: 5s -  - ETA: 0s - loss: \n",
      "Epoch 2/10\n",
      "700/700 [==============================] - 37s 53ms/step - loss: 0.1509 - val_loss: 0.0228\n",
      "Epoch 3/10\n",
      "700/700 [==============================] - 37s 53ms/step - loss: 0.0220 - val_loss: 0.0207- ET - ETA: 0s - loss: 0.022\n",
      "Epoch 4/10\n",
      "700/700 [==============================] - 37s 53ms/step - loss: 0.0218 - val_loss: 0.0222\n",
      "Epoch 5/10\n",
      "700/700 [==============================] - 37s 53ms/step - loss: 0.0263 - val_loss: 0.0212s - loss - ETA: 0s - los\n",
      "Epoch 6/10\n",
      "700/700 [==============================] - 37s 53ms/step - loss: 0.0226 - val_loss: 0.0207\n",
      "Epoch 7/10\n",
      "700/700 [==============================] - 37s 53ms/step - loss: 0.0223 - val_loss: 0.0208\n",
      "Epoch 8/10\n",
      "700/700 [==============================] - 37s 53ms/step - loss: 0.0223 - val_loss: 0.0211\n",
      "Epoch 9/10\n",
      "700/700 [==============================] - 37s 53ms/step - loss: 0.0221 - val_loss: 0.02060 - ETA: 1s - loss: - ETA: 0s\n",
      "Epoch 10/10\n",
      "700/700 [==============================] - 37s 53ms/step - loss: 0.0220 - val_loss: 0.0206\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(images_array_train, scores_array_train, epochs=10, batch_size=2, validation_data=(images_array_val, scores_array_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc48629d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'val_loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAvDElEQVR4nO3de3xcZ33n8c9vRqP7WL7JkiMnseNYckyuxARSKNs2BWIuCduGEEIgYVlCX7vZtluaJdml0GZvdLdbWJaUkpZsuZWQBtimi2kC5bLsQsGOCUmMJV8SJ5HtseWrRpJ1mZnf/nHOyGN5dPWcmZH0fb9eeunc55mxdb5znuec5zF3R0REZKJYpQsgIiLVSQEhIiJFKSBERKQoBYSIiBSlgBARkaIUECIiUpQCQqQEzOyvzOw/zHDb/Wb26+d7HJGoKSBERKQoBYSIiBSlgJBFI6zaudfMnjGzQTP7nJm1mdm3zCxtZt8xs2UF299kZjvN7KSZfd/MLitYd42Z7Qj3+ypQP+G13mpmT4f7/sjMrpxjmT9gZnvN7LiZPW5mF4TLzcw+YWZHzKzfzJ41s8vDdW82s1+EZTtgZr8/pw9MFj0FhCw2vwm8AegE3gZ8C/i3QCvB38NvA5hZJ/AV4HfDdVuBvzOzWjOrBf4X8EVgOfA34XEJ970GeBj4ILAC+CzwuJnVzaagZvZrwH8GbgVWAy8Cj4Sr3wi8PnwfLeE2x8J1nwM+6O5J4HLgu7N5XZE8BYQsNv/D3Q+7+wHgh8BP3P1n7j4MfAO4JtzuncA33f3b7j4G/AnQAPwS8BogAXzS3cfc/TFgW8Fr3A181t1/4u5Zd/88MBLuNxvvBh529x3uPgLcD1xvZmuBMSAJbATM3Xe5+6FwvzFgk5ktcfcT7r5jlq8rAiggZPE5XDB9ush8czh9AcE3dgDcPQe8DHSE6w742T1dvlgwfTHwobB66aSZnQQuDPebjYllGCC4Suhw9+8CnwYeBI6Y2UNmtiTc9DeBNwMvmtkPzOz6Wb6uCKCAEJnMQYITPRDU+ROc5A8Ah4COcFneRQXTLwP/0d2XFvw0uvtXzrMMTQRVVgcA3P1T7n4tsImgqunecPk2d78ZWEVQFfboLF9XBFBAiEzmUeAtZnaDmSWADxFUE/0I+DGQAX7bzBJm9hvAdQX7/gXwW2b26rAxucnM3mJmyVmW4SvA+8zs6rD94j8RVIntN7NXhcdPAIPAMJAL20jebWYtYdVYP5A7j89BFjEFhEgR7t4D3AH8D+AoQYP229x91N1Hgd8A7gKOE7RXfL1g3+3ABwiqgE4Ae8NtZ1uG7wB/AHyN4KplPXBbuHoJQRCdIKiGOgb813Dde4D9ZtYP/BZBW4bIrJkGDBIRkWJ0BSEiIkUpIEREpCgFhIiIFKWAEBGRomoqXYBSWblypa9du7bSxRARmVeeeuqpo+7eWmzdggmItWvXsn379koXQ0RkXjGzFydbpyomEREpSgEhIiJFKSBERKSoBdMGUczY2Bi9vb0MDw9XuiiRq6+vZ82aNSQSiUoXRUQWiAUdEL29vSSTSdauXcvZHW8uLO7OsWPH6O3tZd26dZUujogsEAu6iml4eJgVK1Ys6HAAMDNWrFixKK6URKR8FnRAAAs+HPIWy/sUkfJZ8AExX+TcOT44gnrXFZFqoYCI2MmTJ/mzP/uzabdLD4/Re+I06eEMAG9+85s5efJkxKUTEZmcAiJikwVEJpM5a/70WDDo13AmC8DWrVtZunRp5OUTEZnMgr6LqRrcd9997Nu3j6uvvppEIkF9fT3Lli2ju7ub3bt38/a3v52XX36Z9OAQt73vg3zgA3cDZ7oOGRgYYMuWLbzuda/jRz/6ER0dHfzt3/4tDQ0NFX5nIrLQLZqA+KO/28kvDvaX9JibLljCx972iim3+fjHP85zzz3H008/zfe//33e8pa38Nxzz43fjvrwww+zfPlyfv7CYX7zxl/hrTe9nQuXrz3rGHv27OErX/kKf/EXf8Gtt97K1772Ne64446SvhcRkYlUxVRm11133VnPKnzqU5/iqquu4ta33MDhQwfZvWfvOQ3V69at4+qrrwbg2muvZf/+/WUssYgsVovmCmK6b/rl0tTUND79/e9/n+985zv8ww9+yMGBHHe/822MjAwzksmdtU9dXd34dDwe5/Tp02Urr4gsXrqCiFgymSSdThddd+rUKZYtW0YsUccLe3fz9FPbABgZy5aziCIiRS2aK4hKWbFiBa997Wu5/PLLaWhooK2tbXzdjTfeyJ//+Z/z6ldexYXr1vPq17wGgNMTriBERCrBFsqDWZs3b/aJAwbt2rWLyy67rEIlmrnn+wbIOVy6qpmeVJr6RIyLVzRNv+ME8+X9ikj1MLOn3H1zsXWqYqowd2d4LEd9TfBPUZ+IMTymKwgRqTwFRIVlck4ml6O+Ng5AXSLOaCZLLrcwruxEZP5SQFTYcNggPX4FURPDgZGMGqpFpLIUEBWWr06qT8TP+q1qJhGpNAVEhQ2PZUnEY9TEg3+KupoYZjbeJ5OISKVEGhBmdqOZ9ZjZXjO7r8j615vZDjPLmNktRdYvMbNeM/t0lOWspOGxLHU1Z/4ZzIy6GjVUi0jlRRYQZhYHHgS2AJuAd5nZpgmbvQTcBfz1JIf598D/iaqM5TBVd9/uznAmR0NYrZRXn4gzPJblk5/8JENDQ+UopojIOaK8grgO2Ovuz7v7KPAIcHPhBu6+392fAc75umxm1wJtwJMRljFyUwXESCaHu4+3O+TVJ2KMZXMKCBGpqCifpO4AXi6Y7wVePZMdzSwG/DfgDuDXp9jubuBugIsuumjOBY1SYXffb3jDG1i1ahWPPvooIyMjbHnrTbz7X/w+2dHTvOXWf0pvby/ZbJYPffh+ul/o5eDBg/zqr/4qK1eu5Hvf+16l34qILDLV2tXGvwC2unvvVGMtu/tDwEMQPEk95RG/dR+kni1lGaH9Ctjy8Sk3Kezu+8knn+Sxxx7jpz/9Ke7Om978Vjb+44/o8SEuuOACvvnNbwLQd+w4156O8ZXP/Rnf+973WLlyZWnLLSIyA1FWMR0ALiyYXxMum4nrgXvMbD/wJ8B7zWzqM/E88OSTT/Lkk09yzTXX8MpXvpI9PT0cfOkFrrrqSr797W/z4Q9/mB/+8IesXL6MuBkLpBcUEZmnoryC2AZsMLN1BMFwG3D7THZ093fnp83sLmCzu59zF9SsTPNNvxzcnfvvv58PfvCDAHSn+mlIxLl4RRM7duxg69atfOQjH+GGG27g9t/6PZQPIlJJkV1BuHsGuAd4AtgFPOruO83sATO7CcDMXmVmvcA7gM+a2c6oylMphd19v+lNb+Lhhx9mYGCAbM55+eVe0iePcfDgQRobG7njjju499572bFjB/WJGI1NzfT3l3YUPBGRmYq0DcLdtwJbJyz7aMH0NoKqp6mO8VfAX0VQvLIo7O57y5Yt3H777Vx//fXk3Kmpa+TzX/gCz+59kXvvvZdYLEYikeAzn/kM9Yk4v3H7ndy4ZQsdF1ygRmoRKTt1910hxwZHOHDiNBvbk9TWxM9ZPzA8xvNHB1m3solkfWJGx6zm9ysi1UndfVeh4bEcMTMS8eL/BOqTSUQqTQFRIcNjWeoTcSa7jbcmHqMmFhvv7VVEpNwWfEBUYxVaMEhQlvrE1B9/fSI2426/q/F9isj8tqADor6+nmPHjlXdyTOTdbK5c7vYmCjokyk3bfndnWPHjlFfX1/KYorIIletT1KXxJo1a+jt7aWvr6/SRTnL8FiWowOj5E7U0lekgTpvcCTDiaEx/ETdeHfgk6mvr2fNmilvCBMRmZUFHRCJRIJ169ZVuhjn+OwP9vGfv/UiT3/0DSxtrJ10u5+9dIIPPPIjHnrPtbzxsvYyllBEZIFXMVWr7lSa9iX1U4YDwIa2JAC7D6fLUSwRkbMoICqgO5Wmqz057XbNdTWsWdZAd0oBISLlp4Aos7Fsjn1HBti4evqAAOhqS+oKQkQqQgFRZi8cHWQ0m2PjDK4gALrakzzfN8hoRg/MiUh5KSDKLF9d1NW2ZEbbd7UnyeSc548ORFksEZFzKCDKrCfVTzxmrF/VNKPtO8OG6h61Q4hImSkgyqwnlWZ9axN1Uzz/UGh9azM1MVM7hIiUnQKizHYdStPVPrPqJYDamhjrVjbRk1IVk4iUlwKijNLDYxw4eXrGDdR5ne1Jeg5r4CARKS8FRBnlq4lmGxAb25K8fPw0gyOZKIolIlKUAqKMdh0K72CawxUEwJ4jqmYSkfJRQJRRTypNsq6GjqUNs9qva/xOJlUziUj5KCDKqCfsYmOyQYImc+HyRuoTMTVUi0hZKSDKxN3ZleqfdfUSQDxmdKrLDREps0gDwsxuNLMeM9trZvcVWf96M9thZhkzu6Vg+dVm9mMz22lmz5jZO6MsZzkcOjVMejgz6wbqvM62JD0KCBEpo8gCwsziwIPAFmAT8C4z2zRhs5eAu4C/nrB8CHivu78CuBH4pJktjaqs5ZB/Enrj6pk/A1Goqy1JX3qE44OjpSyWiMikoryCuA7Y6+7Pu/so8Ahwc+EG7r7f3Z8BchOW73b3PeH0QeAI0BphWSO3K2xgznedMVv5qil1uSEi5RJlQHQALxfM94bLZsXMrgNqgX1F1t1tZtvNbHu1DSs6UU8qzQUt9bQ0JOa0fz4g1A4hIuVS1Y3UZrYa+CLwPnc/p79rd3/I3Te7++bW1uq+wOhJpedcvQSwKllHS0NCgweJSNlEGRAHgAsL5teEy2bEzJYA3wT+nbv/Y4nLVlajmRz7+gbmdAdTnplp8CARKasoA2IbsMHM1plZLXAb8PhMdgy3/wbwBXd/LMIylsXzRwcYy/qc72DK62pPsjuVxt1LVDIRkclFFhDungHuAZ4AdgGPuvtOM3vAzG4CMLNXmVkv8A7gs2a2M9z9VuD1wF1m9nT4c3VUZY1avmH5fK4gIOhyIz2S4dCp4VIUS0RkSjVRHtzdtwJbJyz7aMH0NoKqp4n7fQn4UpRlK6fuVJpE3LhkZfN5HaerYPCgC2bZXYeIyGxVdSP1QtF9qJ/1rc3U1pzfxz0eEGqHEJEyUECUQb4PpvPV0pigfUk9u3Unk4iUgQIiYqdOj3Hw1DAbZzGK3FQ625O61VVEykIBEbHxLjZKcAWRP87evgEy2XMeCxERKSkFRMTyYziUoooJgq46RjM5Xjw+VJLjiYhMRgERse5UmiX1NaxuqS/J8QrvZBIRiZICImLdqTQb25fMepCgyVy6qhkzBYSIRE8BESF3Z3eJ7mDKa6iNs3ZFk7rcEJHIKSAidODkadIjGTauLl1AAHS2NetZCBGJnAIiQqW+gymvqy3J/qODDI9lS3pcEZFCCogI5Z9XmOsgQZPpal9CzmHvkYGSHldEpJACIkLdqTQdSxtI1s9tkKDJdLUHfTqpHUJEoqSAiFBPqp/LStz+AHDxiiZq4zHdySQikVJARGQkk2Vf32BJ72DKS8RjrF+lhmoRiZYCIiL7jgySzTldJeqDaaKutmZ12icikVJARKTncNDFxmURXEFA0GnfwVPD9A+PRXJ8EREFRES6D6WpjcdYu7IpkuPnu9zQVYSIREUBEZHuVJr1q5pJxKP5iPNtG2qHEJGoKCAi0pNKR1a9BNCxtIGm2riuIEQkMgqICJwcGiXVPxzJHUx5ZqbBg0QkUgqICORP2lEGBARdeOw+nMbdI30dEVmcIg0IM7vRzHrMbK+Z3Vdk/evNbIeZZczslgnr7jSzPeHPnVGWs9TyD7BdtjqaW1zzOtuSnBgao29gJNLXEZHFKbKAMLM48CCwBdgEvMvMNk3Y7CXgLuCvJ+y7HPgY8GrgOuBjZrYsqrKWWncqzdLGBKuSdZG+zpk7mdQnk4iUXpRXENcBe939eXcfBR4Bbi7cwN33u/szwMQBlt8EfNvdj7v7CeDbwI0RlrWkulP9dLUlSzZI0GQ6wyqs7nBYUxGRUooyIDqAlwvme8NlJdvXzO42s+1mtr2vr2/OBS2lXC4YJKjUXXwXs7K5jpXNteq0T0QiMa8bqd39IXff7O6bW1tbK10cIBgkaHA0y8aI2x/yOtuS9BxWFZOIlF6UAXEAuLBgfk24LOp9K2rXoaC6J+o7mPI625LsOZwml9OdTCJSWlEGxDZgg5mtM7Na4Dbg8Rnu+wTwRjNbFjZOvzFcVvV6IhokaDIb25MMjWbpPXG6LK8nIotHZAHh7hngHoIT+y7gUXffaWYPmNlNAGb2KjPrBd4BfNbMdob7Hgf+PUHIbAMeCJdVve7DaS5a3khzXU1ZXq9TXW6ISEQiPYu5+1Zg64RlHy2Y3kZQfVRs34eBh6MsXxS6D/WXrXoJzlyp9KT6ecOmtrK9rogsfPO6kbraDI9l2X9sqCx3MOU119WwZlmDGqpFpOQUECW098gA2ZyzMaJBgibT1ZZUp30iUnIKiBIqVx9ME3W2J9nXN8BoZuLzhiIic6eAKKGeVD+1NTHWrmgs6+t2tSXJ5JwXjg6W9XVFZGFTQJRQdypNZ1szNRENEjQZDR4kIlFQQJRQdypNV1t52x8ALmltIh4ztUOISEkpIErk+OAofemRst7BlFdXE2fdyiYNHiQiJaWAKJF8j6rlbqDO6woHDxIRKRUFRInku9jYuLpCAdGW5KXjQwyNZiry+iKy8CggSqT7UJrlTbW0Nkc7SNBk8k9U79EDcyJSIjMKCDP7HTNbYoHPhcOEvjHqws0n3YfTZRkkaDL5to8etUOISInM9Arin7l7P0GvqsuA9wAfj6xU88z4IEEVql4CuHB5I/WJmG51FZGSmWlA5L8Wvxn4orvvLFi26L10fIjTY9mK3MGUF48ZG1apoVpESmemAfGUmT1JEBBPmFmSc8eRXrTOdLFR/mcgCnW2JXWrq4iUzEwD4v3AfcCr3H0ISADvi6xU80xPKo0ZdLY1V7QcG9uT9KVHOD44WtFyiMjCMNOAuB7ocfeTZnYH8BHgVHTFml+6U/1cvLyRxtryDBI0mfzgQapmEpFSmGlAfAYYMrOrgA8B+4AvRFaqeaYnla7YA3KFutoUECJSOjMNiIy7O3Az8Gl3fxCo/BmxCpwezbL/2GDZx4Aopm1JHS0NCbVDiEhJzLROJG1m9xPc3vrLZhYjaIdY9PYcSZNzKnoHU56ZafAgESmZmV5BvBMYIXgeIkUwjvR/jaxU80ilBgmaTGd7Mz2H0wQXfCIiczejgAhD4ctAi5m9FRh2d7VBELQ/1CdiXLyiqdJFAYJ2iPRwhkOnhitdFBGZ52ba1catwE+BdwC3Aj8xs1uiLNh80ZNK09mWJB6rjucG889i6IlqETlfM61i+ncEz0Dc6e7vBa4D/mC6nczsRjPrMbO9ZnZfkfV1ZvbVcP1PzGxtuDxhZp83s2fNbFfY/lGVulP943cPVYP8sxhqhxCR8zXTgIi5+5GC+WPT7WtmceBBYAuwCXiXmW2asNn7gRPufinwCeCPw+XvAOrc/QrgWuCD+fCoJkcHRjg6MFo17Q8ASxtraVtSp077ROS8zTQg/t7MnjCzu8zsLuCbwNZp9rkO2Ovuz7v7KPAIwW2yhW4GPh9OPwbcYEF3qA40mVkN0ACMAv0zLGvZ5E/Cl62u/C2uhbral6iKSUTO20wbqe8FHgKuDH8ecvcPT7NbB/BywXxvuKzoNu6eIXg6ewVBWAwCh4CXgD9x9+MTX8DM7jaz7Wa2va+vbyZvpaR2HarsKHKT6WprZs+RAbI53ckkInM3474h3P1rwNciLEuh64AscAFB9+I/NLPvuPvzE8r0EEFwsXnz5rKfDXtSaVY217KyQoMETaazLcloJseLxwa5pLWy/UOJyPw1XTtC2sz6i/ykzWy6Kp8DwIUF82vCZUW3CauTWgjaN24H/t7dx8K2j/8HbJ752yqPnsPpqniCeqJ8mdQOISLnY8qAcPekuy8p8pN09+nOjNuADWa2zsxqgduAxyds8zhwZzh9C/DdsEuPl4BfAzCzJuA1QPfs3lq0sjmvmj6YJrp0VTNmutVVRM5PZGNSh20K9wBPALuAR919p5k9YGY3hZt9DlhhZnuB3yPoUhyCu5+azWwnQdD8T3d/JqqyzsWLxwYZyeSqMiAaauNcvLxRnfaJyHmJtH9qd9/KhLud3P2jBdPDBLe0TtxvoNjyajJ+B1MVVjGBBg8SkfMX2RXEQrcrlSZmsKHCgwRNZmN7kv1HBxkey1a6KCIyTykg5qgn1c/aFU3UJ+KVLkpRne1Jcg77+gYqXRQRmacUEHPUk0qzcXX1tT/kafAgETlfCog5GBrN8OLxIbraqrP9AWDtyiZq4zG1Q4jInCkg5mD34QHcq+8J6kKJeIxLWpvUaZ+IzJkCYg56UsEzgtUwitxUutqT7D6sNggRmRsFxBx0p9I0JOJctLyx0kWZUmdbkgMnT9M/PFbpoojIPKSAmIPuQ2k625PEqmSQoMnkr3D2qKFaROZAATFL7h70wVRFgwRNpjMsY09K1UwiMnsKiFnqGxjh+OBoVd/imtextIGm2vh4m4mIyGwoIGap+1BQXVPNdzDlxWJGZ3tSnfaJyJwoIGYp3wdTNXbzXUxXW5KeVJqgk1wRkZlTQMxSdyrNqmQdy5tqK12UGelsS3JiaIyjA6OVLoqIzDMKiFnqTvXPi+qlvPydTBo8SERmSwExC5lsjj1HBqr+AblCnfmAUDuEiMySAmIW9h8bYjSTmzftDwArm+tY0VSrLjdEZNYUELOQr6aZT1VMEA4epCsIEZklBcQsdKf6iceMS1dV5yBBk+lqT7LncJpcTncyicjMKSBmoTuVZu2KxqodJGgyXe1JhkazHDh5utJFEZF5RAExC8EgQfOn/SHvTJcbqmYSkZlTQMzQwEiGl44PzYs+mCbqDMfN1p1MIjIbkQaEmd1oZj1mttfM7iuyvs7Mvhqu/4mZrS1Yd6WZ/djMdprZs2ZWH2VZp5MfunO+NVADJOsTdCxt0BWEiMxKZAFhZnHgQWALsAl4l5ltmrDZ+4ET7n4p8Angj8N9a4AvAb/l7q8AfgWo6KAG+ZPrZfOwignygwcpIERk5qK8grgO2Ovuz7v7KPAIcPOEbW4GPh9OPwbcYGYGvBF4xt1/DuDux9w9G2FZp9V9qJ+m2jgdSxsqWYw562pPsq9vgLFsrtJFEZF5IsqA6ABeLpjvDZcV3cbdM8ApYAXQCbiZPWFmO8zs3xR7ATO728y2m9n2vr6+kr+BQt2p+TFI0GS62pKMZZ0Xjg5WuigiMk9UayN1DfA64N3h739qZjdM3MjdH3L3ze6+ubW1NbLCjA8SNI+eoJ5IdzKJyGxFGRAHgAsL5teEy4puE7Y7tADHCK42/o+7H3X3IWAr8MoIyzqlw/0jnBwam1d9ME20flUT8ZipHUJEZizKgNgGbDCzdWZWC9wGPD5hm8eBO8PpW4DvejBwwRPAFWbWGAbHPwF+EWFZp9Qdjsg2H+9gyquribNuZRPduoIQkRmqierA7p4xs3sITvZx4GF332lmDwDb3f1x4HPAF81sL3CcIERw9xNm9qcEIePAVnf/ZlRlnc6ZQYLmb0BA0A7x3MFTlS6GiMwTkQUEgLtvJageKlz20YLpYeAdk+z7JYJbXSuuJ5WmfUk9SxvnxyBBk+lsS7L1uUMMjWZorI30n15EFoBqbaSuKrtS6XldvZTX1Z7EHfYcHqh0UURkHlBATGMsm2PfPBskaDJdGjxIRGZBATGN/UcHGc3m2Lh6/gfERcsbqauJafAgEZkRBcQ0duUHCWqbv89A5MVjxoa2Zl1BiMiMKCCm0RMOErR+VVOli1ISXW1L9LCciMyIAmIaPak061ubqKuZX4METaarvZkj6RFODI5WuigiUuUUENPYdShN1zzuYmOifJcbeqJaRKajgJhCeniMAydPL4g7mPLy/UmpHUJEpqOAmEL+W/ZCCoi2JXUsqa9RO4SITEsBMYVdh+bvKHKTMTMNHiQiM6KAmEJPKk2yrmbeDhI0ma72JN2pNEG/iCIixSkgptATdrERDHK3cHS1JUkPZ0j1D1e6KCJSxRQQk3B3ulP9C6p6KU+DB4nITCggJnHo1DD9w5kF1UCdlw89tUOIyFQUEJMYHwNi9cJ5BiJvaWMtbUvqNHiQiExJATGJ/MkzXx2z0HS26U4mEZmaAmIS3al+Lmipp6UhUemiRKKrLcmewwNkc7qTSUSKU0BMomeBDBI0ma72JCOZHC8eG6x0UUSkSikgihjN5NjXN7Ag2x/y1FAtItNRQBTx/NEBxrK+IO9gyrt0VTNm0JPS8KMiUpwCooj8HUwLuYqpsbaGi5Y36gpCRCYVaUCY2Y1m1mNme83sviLr68zsq+H6n5jZ2gnrLzKzATP7/SjLOVF3Kk0iblyysrmcL1t2XW1JulP9lS6GiFSpyALCzOLAg8AWYBPwLjPbNGGz9wMn3P1S4BPAH09Y/6fAt6Iq42S6D/WzvrWZ2pqFfYHV1Z5k/7EhhseylS6KiFShKM+A1wF73f15dx8FHgFunrDNzcDnw+nHgBss7PjIzN4OvADsjLCMRS30O5jyOtuSZHPO8326k0lEzhVlQHQALxfM94bLim7j7hngFLDCzJqBDwN/NNULmNndZrbdzLb39fWVpNCnTo9x8NTw+MA6C1m+Eb7nsKqZRORc1VqH8ofAJ9x9ylts3P0hd9/s7ptbW1tL8sILcZCgyaxd2UQibrqTSUSKqonw2AeACwvm14TLim3Ta2Y1QAtwDHg1cIuZ/RdgKZAzs2F3/3SE5QWC9gdY2Hcw5SXiMda3NutOJhEpKsqA2AZsMLN1BEFwG3D7hG0eB+4EfgzcAnzXg1Fsfjm/gZn9ITBQjnCA4A6mJfU1rG6pL8fLVVxXe5Lt+09UuhgiUoUiq2IK2xTuAZ4AdgGPuvtOM3vAzG4KN/scQZvDXuD3gHNuhS23nlSaje1LFtwgQZPpbEty4ORp0sNjlS6KiFSZKK8gcPetwNYJyz5aMD0MvGOaY/xhJIUr/lr0pNK8/ZqJbekLV1dbvsuNAa69eFmFSyMi1aRaG6kr4sDJ06RHMoui/SFPfTKJyGQUEAXyXWxctnrxBETH0gaaauMaflREzqGAKLDQBwkqJhYzNrQlFRAicg4FRIHuVJqOpQ0k6xfmIEGT6dLociJShAKiQE+qf1FVL+V1tSc5NjhKX3qk0kURkSqigAiNZLLs6xtcVA3UeWqoFpFiFBChfUcGyeacrkXQB9NE+TYXtUOISCEFRCjfYd1li/AKojVZx4qmWl1BiMhZFBCh7lSa2niMtSubKl2UiuhsS47fxSUiAgqIcd2H0qxf1Uwivjg/kq72JHsOp8nlvNJFEZEqsTjPhkX0pNKLsnopr7MtyeBolgMnT1e6KCJSJRQQwMmhUVL9w4vyDqa8/HtXQ7WI5CkgOPME9WIOiM62ZgB61FAtIiEFBGe+NS+GYUYnk6xP0LG0QXcyici4SLv7ni+6U2mWNiZoW1JX6aJUVFe7+mTKG8lk6UmlefbAKWJmbFjVzIZVSVoaF1c3LLK4KSCA7lQ/XW3JRTNI0GQ625L8cE8fY9ncorqbaySTZXdqgGcOnOS5A6d4pvcUuw+nGcuee0dXa7IuDItmLm1Ljk+vaF7cXy5kYVr0AZHLObtTaW65dk2li1JxXe3NjGWd/UcH2bBAe7QdzeTYfTjNM72nePbAKZ49cJKe1JkwaGlIcOWaFv75L1/ClR0tXN7RAsDeIwPsOZJmz+EB9hwZ4Gs7DjAwkhk/7vKmWi4Nw2LDqmY2hOHRmqxb9F88ZP5a9AGR6h9maCzLxtWLt/0hb7zLjcPpBREQ+TAIguAUzx04RfehNKPZHABL6mu4Yk0L73/dJVzR0cKVa1pYs6yh6An9wuWN/OrGVePz7k6qf3g8MPaG4fF3Pz9I//CZ4FhSXzMeFpcWBMfqlnoFh1S9RR8QFyxtYOcfvQnX82Gsb20mHjN6UmneemWlSzM7Y9kgDPJVRM8dOMWugjBI1tdwRUcL73vdWq7oaOGKjhYuWt4455O0mbG6pYHVLQ28vrN1fLm70zcwwt4wOPJXHd/+xWEe2fby+HbNdTWsX9VM56pmNrQF7RuXrmqmY2kDsZiCQ6rDog8IgMbaCn4MmREYGYDRdDDdsBwaV0Cs/G0A9Yk4a1c0Vn1D9Vg2x57DA0EYHDjJswf62XWon9FMGAZ1NVze0cJdrz0TBhevmHsYzIaZsSpZz6pkPb906cqz1h0bGAmrqgbGq6x+sLuPv3mqd3ybhkR8vKrq0jA4Nqxq5sLljcQVHFJmCojZymaCk/nIAIwOnDm5F50fDJelC9ZN2CY3du5rWByaWqG5FZrboGkVNOd/2sJ1bcF8wzIo4Ymvqz3JzoP9DI9lMYO4GfGYRX9ydYfMMJw+CcMnYfgUZEfJxBt4MQ27juV49sgYP0uN8vPUMCOZ4JKvua6GyzuWcOf1F3PFmqVBGCxvrMpv4Sua61jRXMerL1lx1vJTQ2PBlcaRgbDKKs2Pnz/G1392YHyb2poYyboa4jEjEY8Rjxk1seDfJh4zauJGTSw2vqwmbsRjMRIT5vPrE/FweezsY9XEC44xvu2ZbWIxw4DxC27P/zpzCe5+1qqCeS+6Pr9g4vbB9OTrYhaMiBiz/E/hPOP/b2Ph/2MrWB4zC/5/F06bhftTcEwjFjszH48RHsfCYwZ/fvljxCz4fCy/jjPrbMI2sYL9jeB182WslupH8wVSt7J582bfvn377HccGYBtfznFyX7w7GWZ4Zkd1+JQ1wy1yfB3c8Hv5Nnz+emaehg6DoNHYOAwDPSFv48Ey7Kj575OLBEGRkGINIVBMjFg6lumDZNP/cMe/vTbu899O+F/7Hj4BxMf/+Ox8T+yODmSsWFaGKIlNsgSBmlhkCTBdJJBkj5AkkGafYBmH6ApNxj+HiBBkbAsIkeMXE09JJqI1zdjtU2QaITaRkg0Bb9rm85MJ8L58e0mbl+wrqaupIF7PtLDY+NXHPuODDA4miGbczJZJ5tzxnJONpcbn8/knMyE+WzOGcvmyOaKL8uMLw/2y6gvrqpxTsAwIZDCdRhcfeFSvvj+V8/xdewpd99cbF2kVxBmdiPw34E48Jfu/vEJ6+uALwDXAseAd7r7fjN7A/BxoBYYBe519+9GUsjsKHznY4BNOImHv5deWOTk3lSwbGIAJM+c7Et5onEPvlnnQ2PwSBAc+Z98qKSeC6ZzmXOPEa8LA6T17AApuCp5b9dSVvgqbDRNYqw/+Bntpy7TT22mn9qxNHWZNHWZfuozwXRDNk19Nk19dpAYuUnfQo4YQ7EmBmNJBq2JwVgzx+OtDFozaWsibc0M0ETamuinmdraOjqXx1m/1Lg4Ca11GWJjQ8HP6FAQ2GNDMDoEY4PB74G+M9NjQ0HAM4uTnsUKQiMMkEQ9xGshngg+w3giCJKzltUWLE+E83PYNl47vjwZr+OaNS1cc9Gymf8fcQ/er+fC+dzZ8+es46x59xy5XI5MLksu54xlssF8NkMu52SzWchlwXPEPAOexTwXzOeCeTyHeQbLhetymXCbLFYwfdby8e2C41v+OLkshMei8FjkcEuQi9Xg8UQ4Hc7HEmTz81YzvjxnCbLhfNZqgm3C3xmrOWs6a/FgmhpyGO5ONgc59zM/Oci6Bx8pTi78+HMeXCu5e/hPEq4rWJbfJlewTbD8zLEY3+/Msb3g2IWvt2ZZw8z/j89CZAFhZnHgQeANQC+wzcwed/dfFGz2fuCEu19qZrcBfwy8EzgKvM3dD5rZ5cATQEckBW1YBvcfCL49VqDef8bMgrI2LIPWzqm3zeXCMDl8boDkA6a/Fw7ugMG+MycKYCnw7qmOHa+DhqVQvxSaWqDh4mC6vuXM8kmmY7XNNMdiNM/9U5i9fNXV6GDwc1agTLZs6OyQyQxDZjSYz54MvlRkR4Nl2SI/pWbxIDjMip/08/OleCmCb3PxcD6a084sWRxicYjVhNMxwIKgGv/MI7zyyX/+8VqI15wJ+FgiKBOcef3xGpm5zjPL7cPfq68CHp3Lu5tSlFcQ1wF73f15ADN7BLgZKAyIm4E/DKcfAz5tZubuPyvYZifQYGZ17l76QZPNgm/8C0ksBo3Lg59Vl029bS57brXWSD/ULSl+wk/UR1/+UjKDREPw07Ry+u3Plztkx4oEx1hwE0J+OjsyRchMsq17cIVjFvzGzswXTo+vsyLzsfPfNhYPpmM14XR48o7FCqbzy+MTtim2b/7kH5sQBPGC9zeN8bAYC35yY2fP56dzhf82mTPTucw0ywuOkyuczgSfCRSUM6p5Jl+/7OLpP6M5iDIgOoCXC+Z7gYmVZOPbuHvGzE4BKwiuIPJ+E9hRLBzM7G7gboCLLrqodCVfTGLxsKqpFdpeUenSzH9mUFMb/Ej5xOIQC78ISMlUcZ0KmNkrCKqdPlhsvbs/5O6b3X1za2trsU1ERGSOogyIA8CFBfNrwmVFtzGzGqCFoLEaM1sDfAN4r7vvi7CcIiJSRJQBsQ3YYGbrzKwWuA14fMI2jwN3htO3AN91dzezpcA3gfvc/f9FWEYREZlEZAHh7hngHoI7kHYBj7r7TjN7wMxuCjf7HLDCzPYCvwfcFy6/B7gU+KiZPR3+rEJERMpGD8qJiCxiUz0oV9WN1CIiUjkKCBERKUoBISIiRS2YNggz6wNePI9DrOTsB/QWM30WZ9PncTZ9HmcshM/iYncv+iDZggmI82Vm2ydrqFls9FmcTZ/H2fR5nLHQPwtVMYmISFEKCBERKUoBccZDlS5AFdFncTZ9HmfT53HGgv4s1AYhIiJF6QpCRESKUkCIiEhRiz4gzOxGM+sxs71mdt/0eyxcZnahmX3PzH5hZjvN7HcqXaZKM7O4mf3MzP53pctSaWa21MweM7NuM9tlZtdXukyVZGb/Ovw7ec7MvmJm82y4xekt6oAoGDd7C7AJeJeZbapsqSoqA3zI3TcBrwH+5SL/PAB+h6A3YoH/Dvy9u28ErmIRfy5m1gH8NrDZ3S8nGMb7tsqWqvQWdUBQMG62u48C+XGzFyV3P+TuO8LpNMEJoKOypaqccNCqtwB/WemyVJqZtQCvJ+iiH3cfdfeTFS1U5dUADeFgZ43AwQqXp+QWe0AUGzd70Z4QC5nZWuAa4CcVLkolfRL4N0CuwuWoBuuAPuB/hlVuf2lmTZUuVKW4+wHgT4CXgEPAKXd/srKlKr3FHhBShJk1A18Dftfd+ytdnkows7cCR9z9qUqXpUrUAK8EPuPu1wCDnBnga9Exs2UEtQ3rgAuAJjO7o7KlKr3FHhAzGTd7UTGzBEE4fNndv17p8lTQa4GbzGw/QdXjr5nZlypbpIrqBXrdPX9F+RhBYCxWvw684O597j4GfB34pQqXqeQWe0DMZNzsRcPMjKCOeZe7/2mly1NJ7n6/u69x97UE/y++6+4L7hviTLl7CnjZzLrCRTcAv6hgkSrtJeA1ZtYY/t3cwAJstK+pdAEqyd0zZpYfNzsOPOzuOytcrEp6LfAe4Fkzezpc9m/dfWvliiRV5F8BXw6/TD0PvK/C5akYd/+JmT0G7CC4++9nLMBuN9TVhoiIFLXYq5hERGQSCggRESlKASEiIkUpIEREpCgFhIiIFKWAEKkCZvYr6jFWqo0CQkREilJAiMyCmd1hZj81s6fN7LPheBEDZvaJcGyAfzCz1nDbq83sH83sGTP7Rth/D2Z2qZl9x8x+bmY7zGx9ePjmgvEWvhw+oStSMQoIkRkys8uAdwKvdfergSzwbqAJ2O7urwB+AHws3OULwIfd/Urg2YLlXwYedPerCPrvORQuvwb4XYKxSS4heLJdpGIWdVcbIrN0A3AtsC38ct8AHCHoDvyr4TZfAr4ejp+w1N1/EC7/PPA3ZpYEOtz9GwDuPgwQHu+n7t4bzj8NrAX+b+TvSmQSCgiRmTPg8+5+/1kLzf5gwnZz7b9mpGA6i/4+pcJUxSQyc/8A3GJmqwDMbLmZXUzwd3RLuM3twP9191PACTP75XD5e4AfhCP19ZrZ28Nj1JlZYznfhMhM6RuKyAy5+y/M7CPAk2YWA8aAf0kweM514bojBO0UAHcCfx4GQGHvp+8BPmtmD4THeEcZ34bIjKk3V5HzZGYD7t5c6XKIlJqqmEREpChdQYiISFG6ghARkaIUECIiUpQCQkREilJAiIhIUQoIEREp6v8DJ1ag0U6+8fkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d147b99",
   "metadata": {},
   "source": [
    "# HYBRID NEURAL NETWORK WITH MANUAL CLUSTERING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c6575",
   "metadata": {},
   "source": [
    "Now we need to use a manual clustering method to cluster the data of the participants into different clusters, and use this information as input for a hybrid neural network. We will start with the clustering, we can do this using the sklearn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebe3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a941fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 2], [2, 2], [2, 3], [8, 7], [8, 8], [25, 80]])\n",
    "clustering = DBSCAN(eps=3, min_samples=2).fit(X)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306768d3",
   "metadata": {},
   "source": [
    "Now we need to prepare the data so that it can be given as input to the DBSCAN algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4fd528",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_array = np.zeros((34, 1990))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dbeeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20160):\n",
    "    participant_index = int((data[['participant_id']].values[i][0])/3 - 1)\n",
    "    image_index = int((data[['image_id']].values[i][0])/3 - 1)\n",
    "    cluster_array[participant_index][image_index] = data[['response']].values[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e868c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_array[np.isnan(cluster_array)]=0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f5fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_max = np.amax(np.abs(cluster_array))\n",
    "cluster_array = cluster_array/abs_max\n",
    "cluster_array[np.isnan(cluster_array)]=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dcdba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = DBSCAN(eps=12, min_samples=2).fit(cluster_array)\n",
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b523354d",
   "metadata": {},
   "source": [
    "It is not possible to see how these points are located relative to eachother due to the high dimensions of the data. For now we have two apparent clusters. Now we need to create the hybrid network to train the network with these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputA = keras.layers.Input(shape=(1,))\n",
    "inputB = keras.layers.Input(shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa2a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = keras.layers.Dense(8, activation=\"relu\")(inputA)\n",
    "x = keras.layers.Dense(4, activation=\"relu\")(x)\n",
    "x = keras.models.Model(inputs=inputA, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a17fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = keras.models.Sequential()\n",
    "y.add(VGG16(weights = 'imagenet', include_top=False, input_shape=(224,224,3), pooling='avg'))\n",
    "y.add(keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e865472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = keras.layers.concatenate([x.output, y.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c07fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = keras.layers.Dense(19, activation=\"relu\")(combined)\n",
    "z = keras.layers.Dense(1, activation=\"linear\")(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582c36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd43fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94051ed4",
   "metadata": {},
   "source": [
    "Now that the model is created, the data has to be fitted to the input of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_array = np.zeros((20160,1))\n",
    "image_array = np.zeros((20160,224,224,3))\n",
    "clusterinfo_array = np.zeros((20160,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd25194",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20160):\n",
    "    \n",
    "    #ratings\n",
    "    \n",
    "    rating_array[i] = (data[['response']].values[i][0])\n",
    "    \n",
    "    \n",
    "    #images\n",
    "    \n",
    "    image_index = int((data[['image_id']].values[i][0])/3 - 1)\n",
    "    image_location = images[['image_filename']].values[image_index][0]\n",
    "    im = Image.open('Data/Images/Final_resized_selection_batch1/' + image_location)\n",
    "    im = np.array(im)\n",
    "    im = scale_resize_image(im)\n",
    "    image_array[i] = im\n",
    "    \n",
    "    #clusters\n",
    "    \n",
    "    participant_index = int((data[['participant_id']].values[i][0])/3 - 1)\n",
    "    clusterinfo_array[i] = clustering.labels_[participant_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c302e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_unison_shuffled_copies(a, b, c):\n",
    "    assert len(a) == len(b)\n",
    "    assert len(a) == len(c)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p], c[p]\n",
    "\n",
    "rating_array_shuffled, image_array_shuffled, cluster_array_shuffled = three_unison_shuffled_copies(rating_array, image_array, clusterinfo_array)\n",
    "\n",
    "rating_array_train = rating_array_shuffled[:14000]\n",
    "image_array_train = image_array_shuffled[:14000]\n",
    "cluster_array_train = cluster_array_shuffled[:14000]\n",
    "\n",
    "rating_array_val = rating_array_shuffled[14000:15000]\n",
    "image_array_val = image_array_shuffled[14000:15000]\n",
    "cluster_array_val = cluster_array_shuffled[14000:15000]\n",
    "\n",
    "rating_array_test = rating_array_shuffled[15000:]\n",
    "image_array_test = image_array_shuffled[15000:]\n",
    "cluster_array_test = cluster_array_shuffled[15000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bab9b2",
   "metadata": {},
   "source": [
    "With that done the model can be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25736524",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[cluster_array_train, image_array_train], y=rating_array_train, validation_data=([cluster_array_val, image_array_val], rating_array_val), epochs=2, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5615e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d05eb2",
   "metadata": {},
   "source": [
    "# HYBRID NEURAL NETWORK WITH SOM SUBNETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda656d",
   "metadata": {},
   "source": [
    "The last part of this code will be quite similar to the second part, with the expection that the cluster algorithm will be included in the neural network, using a SOM Neural Network for the first input. First we will create the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea0d70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "\n",
    "\n",
    "class SOMLayer(Layer):\n",
    "    \"\"\"\n",
    "    Self-Organizing Map layer class with rectangular topology\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(SOMLayer(map_size=(10,10)))\n",
    "    ```\n",
    "    # Arguments\n",
    "        map_size: Tuple representing the size of the rectangular map. Number of prototypes is map_size[0]*map_size[1].\n",
    "        prototypes: Numpy array with shape `(n_prototypes, latent_dim)` witch represents the initial cluster centers\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, latent_dim)`\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_prototypes)`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, map_size, prototypes=None, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'latent_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('latent_dim'),)\n",
    "        super(SOMLayer, self).__init__(**kwargs)\n",
    "        self.map_size = map_size\n",
    "        self.n_prototypes = map_size[0]*map_size[1]\n",
    "        self.initial_prototypes = prototypes\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "        self.prototypes = None\n",
    "        self.built = False\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert(len(input_shape) == 2)\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=tf.float32, shape=(None, input_dim))\n",
    "        self.prototypes = self.add_weight(shape=(self.n_prototypes, input_dim), initializer='glorot_uniform', name='prototypes')\n",
    "        if self.initial_prototypes is not None:\n",
    "            self.set_weights(self.initial_prototypes)\n",
    "            del self.initial_prototypes\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\"\n",
    "        Calculate pairwise squared euclidean distances between inputs and prototype vectors\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, Tensor with shape `(n_samples, latent_dim)`\n",
    "        Return:\n",
    "            d: distances between inputs and prototypes, Tensor with shape `(n_samples, n_prototypes)`\n",
    "        \"\"\"\n",
    "        # Note: (tf.expand_dims(inputs, axis=1) - self.prototypes) has shape (n_samples, n_prototypes, latent_dim)\n",
    "        d = tf.reduce_sum(tf.square(tf.expand_dims(inputs, axis=1) - self.prototypes), axis=2)\n",
    "        return d\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert(input_shape and len(input_shape) == 2)\n",
    "        return input_shape[0], self.n_prototypes\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'map_size': self.map_size}\n",
    "        base_config = super(SOMLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e178e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputA = keras.layers.Input(shape=(1990))\n",
    "inputB = keras.layers.Input(shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d59058",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = SOMLayer((1990,1))(inputA)\n",
    "x = keras.models.Model(inputs=inputA, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef711179",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = keras.models.Sequential()\n",
    "y.add(VGG16(weights = 'imagenet', include_top=False, input_shape=(224,224,3), pooling='avg'))\n",
    "y.add(keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de8f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = keras.layers.concatenate([x.output, y.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d452df97",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = keras.layers.Dense(19, activation=\"relu\")(combined)\n",
    "z = keras.layers.Dense(1, activation=\"linear\")(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4256a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362ad0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8546bd6",
   "metadata": {},
   "source": [
    "Now the data has to be fitted to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90469b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_array = np.zeros((20160,1))\n",
    "image_array = np.zeros((20160,224,224,3))\n",
    "clusterinfo_array = np.zeros((20160,1990))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0bdb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20160):\n",
    "    \n",
    "    #ratings\n",
    "    \n",
    "    rating_array[i] = (data[['response']].values[i][0])\n",
    "    \n",
    "    \n",
    "    #images\n",
    "    \n",
    "    image_index = int((data[['image_id']].values[i][0])/3 - 1)\n",
    "    image_location = images[['image_filename']].values[image_index][0]\n",
    "    im = Image.open('Data/Images/Final_resized_selection_batch1/' + image_location)\n",
    "    im = np.array(im)\n",
    "    im = scale_resize_image(im)\n",
    "    image_array[i] = im\n",
    "    \n",
    "    #clusters\n",
    "    \n",
    "    participant_index = int((data[['participant_id']].values[i][0])/3 - 1)\n",
    "    clusterinfo_array[i] = cluster_array[participant_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7809664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_unison_shuffled_copies(a, b, c):\n",
    "    assert len(a) == len(b)\n",
    "    assert len(a) == len(c)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p], c[p]\n",
    "\n",
    "rating_array_shuffled, image_array_shuffled, cluster_array_shuffled = three_unison_shuffled_copies(rating_array, image_array, clusterinfo_array)\n",
    "\n",
    "rating_array_train = rating_array_shuffled[:14000]\n",
    "image_array_train = image_array_shuffled[:14000]\n",
    "cluster_array_train = cluster_array_shuffled[:14000]\n",
    "\n",
    "rating_array_val = rating_array_shuffled[14000:15000]\n",
    "image_array_val = image_array_shuffled[14000:15000]\n",
    "cluster_array_val = cluster_array_shuffled[14000:15000]\n",
    "\n",
    "rating_array_test = rating_array_shuffled[15000:]\n",
    "image_array_test = image_array_shuffled[15000:]\n",
    "cluster_array_test = cluster_array_shuffled[15000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af69ba1",
   "metadata": {},
   "source": [
    "Now we need to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4470c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[cluster_array_train, image_array_train], y=rating_array_train, validation_data=([cluster_array_val, image_array_val], rating_array_val), epochs=2, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c72ac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8a24a",
   "metadata": {},
   "source": [
    "# HYBRID NEURAL NETWORK WITH SOM PRETRAINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def som_loss(weights, distances):\n",
    "    \"\"\"\n",
    "    SOM loss\n",
    "    # Arguments\n",
    "        weights: weights for the weighted sum, Tensor with shape `(n_samples, n_prototypes)`\n",
    "        distances: pairwise squared euclidean distances between inputs and prototype vectors, Tensor with shape `(n_samples, n_prototypes)`\n",
    "    # Return\n",
    "        SOM reconstruction loss\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(tf.reduce_sum(weights*distances, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d624999",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputA = keras.layers.Input(shape=(1990))\n",
    "x = SOMLayer((1990,1))(inputA)\n",
    "x = keras.models.Model(inputs=inputA, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4a31c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.compile(loss=som_loss, optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef6b5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.fit(x=cluster_array_train, y=cluster_array_train, epochs=2, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a11d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputB = keras.layers.Input(shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4aab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = keras.models.Sequential()\n",
    "y.add(VGG16(weights = 'imagenet', include_top=False, input_shape=(224,224,3), pooling='avg'))\n",
    "y.add(keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37c9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = keras.layers.concatenate([x.output, y.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3783e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = keras.layers.Dense(19, activation=\"relu\")(combined)\n",
    "z = keras.layers.Dense(1, activation=\"linear\")(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=[x.input, y.input], outputs=z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_absolute_percentage_error\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47ed0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[cluster_array_train, image_array_train], y=rating_array_train, validation_data=([cluster_array_val, image_array_val], rating_array_val), epochs=2, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9a653b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
